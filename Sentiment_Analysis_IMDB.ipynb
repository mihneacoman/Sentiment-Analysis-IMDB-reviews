{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mihneacoman/Sentiment-Analysis-IMDB-reviews/blob/main/Sentiment_Analysis_IMDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Importing the csv file and renaming the columns.**\n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9qbag0OLTKs0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DQUblQtntLx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path = \"/content/drive/MyDrive/Colab Notebooks/Proiect IMDB/\"\n",
        "df = pd.read_csv(path+'IMDB Dataset.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the required libraries."
      ],
      "metadata": {
        "id": "r5CPFC4PSuwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import spacy\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import wordnet as wn\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "hUl_8iUQSuIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.rename(columns={'review': 'text'})\n",
        "df['sentiment'] = df['sentiment'].map({'negative':-1,'positive':1})\n",
        "df"
      ],
      "metadata": {
        "id": "Xl17vZjSpRAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Choosing a random sample of2000 **\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qH7bnCy_L9Hc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['sentiment'].value_counts())\n",
        "\n",
        "# Impart setul de date dupa sentiment (negativ si pozitiv)\n",
        "df_negative = df[df['sentiment'] == -1]\n",
        "df_positive = df[df['sentiment'] == 1]\n",
        "\n",
        "min_samples = 2000\n",
        "\n",
        "# Selectam random 2000 de exemple din fiecare sentiment\n",
        "df_negative_sampled = df_negative.sample(n=min_samples)\n",
        "df_positive_sampled = df_positive.sample(n=min_samples)\n",
        "\n",
        "# Le combinam intr-un singur dataframe\n",
        "df_sampled = pd.concat([df_negative_sampled, df_positive_sampled])\n",
        "\n",
        "# Amestecam randurile\n",
        "df_sampled = df_sampled.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "print(df_sampled['sentiment'].value_counts())\n",
        "\n",
        "df_sampled\n"
      ],
      "metadata": {
        "id": "Ek9YHFGbn06n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "**Analiza setului de date**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HqKP5rb5Su-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_map = {-1: \"negativ\", 1: \"pozitiv\"}\n",
        "sentiment_counts = df[\"sentiment\"].value_counts().sort_index()\n",
        "sentiment_counts.index = sentiment_counts.index.map(label_map)\n",
        "sentiment_counts.plot(kind=\"bar\", color=[\"red\", \"green\"])"
      ],
      "metadata": {
        "id": "WcuaKttFSzw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "review_len = pd.Series([len(review.split()) for review in df['text']])\n",
        "\n",
        "fig = plt.figure(figsize=(14,7))\n",
        "df['length'] = df.text.str.split().apply(len)\n",
        "ax1 = fig.add_subplot(122)\n",
        "sns.histplot(df[df['sentiment']==1]['length'], ax=ax1,color='magenta')\n",
        "describe = df.length[df.sentiment==1].describe().to_frame().round(2)\n",
        "\n",
        "ax2 = fig.add_subplot(121)\n",
        "ax2.axis('off')\n",
        "font_size = 14\n",
        "bbox = [0, 0, 1, 1]\n",
        "table = ax2.table(cellText = describe.values, rowLabels = describe.index, bbox=bbox, colLabels=describe.columns)\n",
        "table.set_fontsize(font_size)\n",
        "fig.suptitle('Distribution of text length for positive sentiment reviews.', fontsize=16)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sscTR7WlS5IJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(14,7))\n",
        "df['length'] = df.text.str.split().apply(len)\n",
        "ax1 = fig.add_subplot(122)\n",
        "sns.histplot(df[df['sentiment']==-1]['length'], ax=ax1,color='magenta')\n",
        "describe = df.length[df.sentiment==-1].describe().to_frame().round(2)\n",
        "\n",
        "ax2 = fig.add_subplot(121)\n",
        "ax2.axis('off')\n",
        "font_size = 14\n",
        "bbox = [0, 0, 1, 1]\n",
        "table = ax2.table(cellText = describe.values, rowLabels = describe.index, bbox=bbox, colLabels=describe.columns)\n",
        "table.set_fontsize(font_size)\n",
        "fig.suptitle('Distribution of text length for negative sentiment reviews.', fontsize=16)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dFr0l6g4S8ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**TF_IDF vectorization and logistic regression**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "l50iH-t_4WoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the spacy model for tokenization\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# list of common negations (we wish to keep them)\n",
        "negations = {\"not\", \"no\", \"n't\", \"isn't\", \"aren't\", \"won't\", \"can't\", \"never\", \"nothing\"}\n",
        "\n",
        "def review_to_words(review):\n",
        "\n",
        "    # removing HTML tags and hashtags\n",
        "    review = re.sub(r\"<.*?>\", \" \", review)\n",
        "    review = re.sub(r\"(@[A-Za-z0-9_]+)|(#\\S+)\", \" \", review)\n",
        "\n",
        "    doc = nlp(review.lower())\n",
        "\n",
        "    clean_tokens = []\n",
        "    for token in doc:\n",
        "        # removing punctuation, spaces and stopwords (excluding negations)\n",
        "        if token.is_punct or token.is_space:\n",
        "            continue\n",
        "        if token.is_stop and token.text not in negations:\n",
        "            continue\n",
        "        # lematizare\n",
        "        clean_tokens.append(token.lemma_)\n",
        "\n",
        "    return clean_tokens\n",
        "\n",
        "df_sampled['processed_text'] = df_sampled['text'].apply(review_to_words)\n",
        "\n",
        "# checking that preprocessing worked\n",
        "print(df_sampled[['text', 'processed_text']].head())\n",
        "\n",
        "#transforming tokens into strings\n",
        "df_sampled['processed_text'] = df_sampled['processed_text'].apply(lambda tokens: ' '.join(tokens))\n",
        "\n",
        "#splitting the dataset\n",
        "X_text_train, X_text_test, y_train, y_test = train_test_split(df_sampled['processed_text'], df_sampled['sentiment'], test_size=0.3)\n",
        "\n",
        "# vectorization of X_train and X_test\n",
        "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
        "X_train = vectorizer.fit_transform(X_text_train)\n",
        "\n",
        "X_test = vectorizer.transform(X_text_test)\n",
        "\n",
        "# training the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000, class_weight='balanced', C=1.0)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# predictions and evaluation\n",
        "y_pred_lr = model.predict(X_test)\n",
        "y_true_lr = y_test\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_lr))"
      ],
      "metadata": {
        "id": "GF_kyqXToY9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Verifying a new review.**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "_eW513MKhLo0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_text = \"I did not enjoy this movie\"\n",
        "print(f\"The review is: {new_text}\")\n",
        "tokens = review_to_words(new_text)\n",
        "prop = ' '.join(tokens)\n",
        "\n",
        "X_new = vectorizer.transform([prop])\n",
        "\n",
        "y_pred_new = model.predict(X_new)\n",
        "\n",
        "print(\"Preprocessed review:\", prop)\n",
        "print(\"Prediction for this review:\", y_pred_new)"
      ],
      "metadata": {
        "id": "cpwZDsm3hLav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "False positives and false negatives\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "8nvfWeTG4uSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# resetting the index to match X_test\n",
        "df_test = df_sampled.iloc[y_test.index].copy()\n",
        "df_test[\"true_label\"] = y_test.values\n",
        "df_test[\"predicted_label\"] = y_pred_lr\n",
        "\n",
        "# false positives:\n",
        "false_positives = df_test[(df_test[\"true_label\"] == -1) & (df_test[\"predicted_label\"] == 1)]\n",
        "\n",
        "# false negatives:\n",
        "false_negatives = df_test[(df_test[\"true_label\"] == 1) & (df_test[\"predicted_label\"] == -1)]\n",
        "\n",
        "# printing a few examples\n",
        "print(\"FALSE POSITIVES (predicted 1, true -1):\")\n",
        "print(false_positives[[\"text\", \"true_label\", \"predicted_label\"]].head(5))\n",
        "\n",
        "print(\"\\nFALSE NEGATIVES (predicted -1, true 1):\")\n",
        "print(false_negatives[[\"text\", \"true_label\", \"predicted_label\"]].head(5))\n",
        "#pd.set_option(\"display.max_colwidth\", None)"
      ],
      "metadata": {
        "id": "MgYG9nCpwsVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handling negations."
      ],
      "metadata": {
        "id": "ejylAXol04fw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "#function that replaces negated words with their antonyms\n",
        "def Negation(sentence):\n",
        "\n",
        "    i = 1\n",
        "    while i < len(sentence):\n",
        "        if sentence[i-1] in ['not', \"n't\"]:\n",
        "            antonyms = []\n",
        "\n",
        "            for syn in wn.synsets(sentence[i]):\n",
        "                for lemma in syn.lemmas():\n",
        "                    if lemma.antonyms():\n",
        "                        antonyms.append(lemma.antonyms()[0].name())\n",
        "\n",
        "            if 'bad' in antonyms:\n",
        "                sentence[i] = 'bad'\n",
        "            elif antonyms:\n",
        "                sentence[i] = antonyms[0]\n",
        "            sentence[i-1] = ''\n",
        "        i += 1\n",
        "\n",
        "    return ' '.join([word for word in sentence if word])\n",
        "\n",
        "df_sampled = df_sampled.drop('processed_text', axis=1)\n",
        "\n",
        "df_sampled['processed_text'] = df_sampled['text'].apply(review_to_words)\n",
        "df_sampled['processed_text'] = df_sampled['processed_text'].apply(Negation)\n",
        "\n",
        "X_text_train, X_text_test, y_train, y_test = train_test_split(df_sampled['processed_text'], df_sampled['sentiment'], test_size=0.3)\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
        "X_train = vectorizer.fit_transform(X_text_train)\n",
        "\n",
        "X_test = vectorizer.transform(X_text_test)\n",
        "\n",
        "# training the new model (that handles negations)\n",
        "model = LogisticRegression(max_iter=1000, class_weight='balanced', C=1.0)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# evaluation\n",
        "y_pred_lr = model.predict(X_test)\n",
        "y_true_lr = y_test\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_lr))"
      ],
      "metadata": {
        "id": "FZBP_4NC04Ry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bag of Words + Logistic Regression"
      ],
      "metadata": {
        "id": "hs5TydoXdfDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "X_text_train_bow, X_text_test_bow, y_train_bow, y_test_bow = train_test_split(df_sampled['processed_text'], df_sampled['sentiment'], test_size=0.3)\n",
        "\n",
        "vectorizer = CountVectorizer(max_features=5000, ngram_range=(1, 2))\n",
        "X_train_bow = vectorizer.fit_transform(X_text_train_bow)\n",
        "\n",
        "X_test_bow = vectorizer.transform(X_text_test_bow)\n",
        "\n",
        "model = LogisticRegression(max_iter=1000, class_weight='balanced', C=1.0)\n",
        "model.fit(X_train_bow, y_train_bow)\n",
        "\n",
        "y_pred_bow = model.predict(X_test_bow)\n",
        "y_true_bow = y_test_bow\n",
        "print(\"Accuracy:\", accuracy_score(y_test_bow, y_pred_bow))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test_bow, y_pred_bow))"
      ],
      "metadata": {
        "id": "0rOtZ_Dxde3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF + Naive Bayes"
      ],
      "metadata": {
        "id": "P8Qda6V2ewCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "X_text_train_nb, X_text_test_nb, y_train_nb, y_test_nb = train_test_split(df_sampled['processed_text'], df_sampled['sentiment'], test_size=0.3)\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
        "X_train_nb = vectorizer.fit_transform(X_text_train_nb)\n",
        "\n",
        "X_test_nb = vectorizer.transform(X_text_test_nb)\n",
        "\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train_nb, y_train_nb)\n",
        "\n",
        "y_pred_nb = model.predict(X_test_nb)\n",
        "y_true_nb = y_test_nb\n",
        "print(\"Accuracy:\", accuracy_score(y_test_nb, y_pred_nb))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test_nb, y_pred_nb))"
      ],
      "metadata": {
        "id": "M743zr3Uev0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BoW cu Naive Bayes"
      ],
      "metadata": {
        "id": "jtxt-fv-f6eS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_text_train_nb, X_text_test_nb, y_train_nb, y_test_nb = train_test_split(df_sampled['processed_text'], df_sampled['sentiment'], test_size=0.3)\n",
        "\n",
        "vectorizer = CountVectorizer(max_features=5000, ngram_range=(1, 2))\n",
        "X_train_nb = vectorizer.fit_transform(X_text_train_nb)\n",
        "\n",
        "X_test_nb = vectorizer.transform(X_text_test_nb)\n",
        "\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train_nb, y_train_nb)\n",
        "\n",
        "y_pred_nb = model.predict(X_test_nb)\n",
        "y_true_nb = y_test_nb\n",
        "print(\"Accuracy:\", accuracy_score(y_test_nb, y_pred_nb))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test_nb, y_pred_nb))"
      ],
      "metadata": {
        "id": "efj1sq1Jf6Qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Adding a dependency tree derived feature.**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wTZilUTz4RcV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words(\"english\"))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def clean_text(text):\n",
        "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
        "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
        "    return text.lower()\n",
        "\n",
        "# we extract SVO triplets\n",
        "def extract_dependency_features(text):\n",
        "    doc = nlp(text)\n",
        "    triples = []\n",
        "    for token in doc:\n",
        "        if token.dep_ == \"ROOT\" and token.pos_ == \"VERB\":\n",
        "            subject = [child for child in token.children if \"subj\" in child.dep_]\n",
        "            obj = [child for child in token.children if \"obj\" in child.dep_ or \"attr\" in child.dep_]\n",
        "            if subject and obj:\n",
        "                s = subject[0].lemma_\n",
        "                v = token.lemma_\n",
        "                o = obj[0].lemma_\n",
        "                triples.append(f\"{s}_{v}_{o}\")\n",
        "\n",
        "    return \" \".join(triples)\n",
        "\n",
        "def preprocess_with_dependencies(text):\n",
        "    text_clean = clean_text(text)\n",
        "    tokens = text_clean.split()\n",
        "    tokens_neg_handled = Negation(tokens)\n",
        "    text_neg_handled = tokens_neg_handled\n",
        "    text_lemmas = [lemmatizer.lemmatize(word) for word in text_neg_handled.split() if word not in stop_words]\n",
        "    triple_features = extract_dependency_features(text_neg_handled)\n",
        "    return \" \".join(text_lemmas) + \" \" + triple_features\n",
        "\n",
        "df_sampled['processed_text_tree'] = df_sampled['text'].apply(preprocess_with_dependencies)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_sampled['processed_text_tree'], df_sampled['sentiment'], test_size=0.3)\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "model = LogisticRegression(max_iter=1000, class_weight='balanced', C=1.0)\n",
        "model.fit(X_train_vec, y_train)\n",
        "\n",
        "# Predicții și metrice\n",
        "y_pred_tree = model.predict(X_test_vec)\n",
        "y_true_tree=y_test\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_tree))\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred_tree))"
      ],
      "metadata": {
        "id": "QHtNgCNnAXiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test_tree = df_sampled.iloc[y_test.index].copy()\n",
        "df_test_tree[\"true_label\"] = y_test.values\n",
        "df_test_tree[\"predicted_label\"] = y_pred_tree\n",
        "\n",
        "# False positives\n",
        "false_positives = df_test_tree[(df_test_tree[\"true_label\"] == -1) & (df_test_tree[\"predicted_label\"] == 1)]\n",
        "\n",
        "# False negatives\n",
        "false_negatives = df_test_tree[(df_test_tree[\"true_label\"] == 1) & (df_test_tree[\"predicted_label\"] == -1)]\n",
        "\n",
        "# Printing a few examples\n",
        "print(\"FALSE POSITIVES (predicted 1, true -1):\")\n",
        "print(false_positives[[\"text\", \"true_label\", \"predicted_label\"]].head(5))\n",
        "\n",
        "print(\"\\nFALSE NEGATIVES (predicted -1, true 1):\")\n",
        "print(false_negatives[[\"text\", \"true_label\", \"predicted_label\"]].head(5))"
      ],
      "metadata": {
        "id": "XknMrTRfZh7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "Transformers\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nVWhrZ6AKmFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y transformers\n",
        "!pip install transformers datasets -q\n",
        "from transformers import TrainingArguments\n",
        "import transformers\n",
        "print(transformers.__version__)"
      ],
      "metadata": {
        "id": "s7c65ZSJQ70R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from transformers import DataCollatorWithPadding\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# desabling wandb (external logging)\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# we need the labels to be 0 and 1\n",
        "df_test_bert = df_sampled.copy()\n",
        "df_test_bert['label'] = df_test_bert['sentiment'].map({-1: 0, 1: 1})\n",
        "\n",
        "# train/test split\n",
        "train_df, test_df = train_test_split(df_test_bert, test_size=0.2, stratify=df_test_bert['label'])\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_df[['text', 'label']])\n",
        "test_dataset = Dataset.from_pandas(test_df[['text', 'label']])\n",
        "\n",
        "# tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"text\"], truncation=True, padding=True)\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# model and data collator\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# hyperparameters\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10\n",
        ")\n",
        "\n",
        "# trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# evaluation\n",
        "preds_output = trainer.predict(test_dataset)\n",
        "y_pred_bert = preds_output.predictions.argmax(-1)\n",
        "y_true_bert = preds_output.label_ids\n",
        "\n",
        "df_test_bert = test_df.copy()\n",
        "\n",
        "df_test_bert[\"true_label\"] = y_true_bert\n",
        "df_test_bert[\"predicted_label\"] = y_pred_bert\n",
        "\n",
        "df_test_bert[\"true_label\"] = df_test_bert[\"true_label\"].map({0: -1, 1: 1})\n",
        "df_test_bert[\"predicted_label\"] = df_test_bert[\"predicted_label\"].map({0: -1, 1: 1})\n",
        "\n",
        "print(\"\\nclassification report:\")\n",
        "print(classification_report(y_true_bert, y_pred_bert, target_names=[\"Negative\", \"Positive\"]))"
      ],
      "metadata": {
        "id": "AElKNH8dP6ex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "ConfusionMatrixDisplay.from_predictions(y_true_bert, y_pred_bert, display_labels=[\"Negative\", \"Positive\"])\n",
        "plt.title(\"Confusion Matrix - Transformers (BERT)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "gW73r_k9nhlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training loss plot."
      ],
      "metadata": {
        "id": "eD9I4ttUKMXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "log_history = trainer.state.log_history\n",
        "\n",
        "steps = []\n",
        "losses = []\n",
        "\n",
        "for entry in log_history:\n",
        "    if \"loss\" in entry and \"epoch\" in entry:\n",
        "        steps.append(entry[\"step\"])\n",
        "        losses.append(entry[\"loss\"])\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(steps, losses, marker='o')\n",
        "plt.title(\"Training Loss per Step\")\n",
        "plt.xlabel(\"Step\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5Am0HVxQTOV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Comparing the models.**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "yJjgn-iyRG7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_pred_lr = y_pred_lr.ravel()\n",
        "y_pred_tree = y_pred_tree.ravel()\n",
        "y_pred_bert = y_pred_bert.ravel()\n",
        "\n",
        "report_lr = classification_report(y_true_lr, y_pred_lr, target_names=[\"Negative\", \"Positive\"], output_dict=True)\n",
        "report_tree = classification_report(y_true_tree, y_pred_tree, target_names=[\"Negative\", \"Positive\"], output_dict=True)\n",
        "report_bert = classification_report(y_true_bert, y_pred_bert, target_names=[\"Negative\", \"Positive\"], output_dict=True)\n",
        "\n",
        "def extract_metrics(report, model_name):\n",
        "    rows = []\n",
        "    for cls in [\"Negative\", \"Positive\"]:\n",
        "        row = {\n",
        "            \"Model\": model_name,\n",
        "            \"Class\": cls,\n",
        "            \"Precision\": round(report[cls][\"precision\"], 2),\n",
        "            \"Recall\": round(report[cls][\"recall\"], 2),\n",
        "            \"F1-Score\": round(report[cls][\"f1-score\"], 2),\n",
        "            \"Support\": int(report[cls][\"support\"])\n",
        "        }\n",
        "        rows.append(row)\n",
        "    return rows\n",
        "\n",
        "data = []\n",
        "data += extract_metrics(report_lr, \"Logistic Regression\")\n",
        "data += extract_metrics(report_tree, \"Dependency Tree\")\n",
        "data += extract_metrics(report_bert, \"Transformers\")\n",
        "\n",
        "comparison_df = pd.DataFrame(data)\n",
        "\n",
        "print(comparison_df)"
      ],
      "metadata": {
        "id": "KbM2C-heaOIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "false_positives_lr = df_test[(df_test[\"true_label\"] == -1) & (df_test[\"predicted_label\"] == 1)]\n",
        "false_negatives_lr = df_test[(df_test[\"true_label\"] == 1) & (df_test[\"predicted_label\"] == -1)]\n",
        "\n",
        "false_positives_tree = df_test_tree[(df_test_tree[\"true_label\"] == -1) & (df_test_tree[\"predicted_label\"] == 1)]\n",
        "false_negatives_tree = df_test_tree[(df_test_tree[\"true_label\"] == 1) & (df_test_tree[\"predicted_label\"] == -1)]\n",
        "\n",
        "false_positives_bert = df_test_bert[(df_test_bert[\"true_label\"] == -1) & (df_test_bert[\"predicted_label\"] == 1)]\n",
        "false_negatives_bert = df_test_bert[(df_test_bert[\"true_label\"] == 1) & (df_test_bert[\"predicted_label\"] == -1)]\n",
        "\n",
        "#total number of errors\n",
        "false_pos_lr_count = len(false_positives_lr)\n",
        "false_neg_lr_count = len(false_negatives_lr)\n",
        "false_pos_tree_count = len(false_positives_tree)\n",
        "false_neg_tree_count = len(false_negatives_tree)\n",
        "false_pos_bert_count = len(false_positives_bert)\n",
        "false_neg_bert_count = len(false_negatives_bert)\n",
        "\n",
        "# comparative dataframe\n",
        "comparison_data = {\n",
        "    \"Model\": [\"Logistic Regression\", \"Logistic Regression\",\n",
        "              \"Dependency Tree\", \"Dependency Tree\",\n",
        "              \"Transformers\", \"Transformers\"],\n",
        "    \"Error Type\": [\"False Positives\", \"False Negatives\",\n",
        "                   \"False Positives\", \"False Negatives\",\n",
        "                   \"False Positives\", \"False Negatives\"],\n",
        "    \"Count\": [false_pos_lr_count, false_neg_lr_count,\n",
        "              false_pos_tree_count, false_neg_tree_count,\n",
        "              false_pos_bert_count, false_neg_bert_count]\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "\n",
        "print(comparison_df)\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "ax[0].bar(['LR', 'Tree', 'BERT'],\n",
        "          [false_pos_lr_count, false_pos_tree_count, false_pos_bert_count],\n",
        "          color=['blue', 'orange', 'green'])\n",
        "ax[0].set_title(\"False Positives Comparison\")\n",
        "ax[0].set_ylabel(\"Count\")\n",
        "ax[0].set_xlabel(\"Model\")\n",
        "\n",
        "ax[1].bar(['LR', 'Tree', 'BERT'],\n",
        "          [false_neg_lr_count, false_neg_tree_count, false_neg_bert_count],\n",
        "          color=['blue', 'orange', 'green'])\n",
        "ax[1].set_title(\"False Negatives Comparison\")\n",
        "ax[1].set_ylabel(\"Count\")\n",
        "ax[1].set_xlabel(\"Model\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# pie chart\n",
        "labels = ['LR FP', 'Tree FP', 'BERT FP', 'LR FN', 'Tree FN', 'BERT FN']\n",
        "sizes = [false_pos_lr_count, false_pos_tree_count, false_pos_bert_count,\n",
        "         false_neg_lr_count, false_neg_tree_count, false_neg_bert_count]\n",
        "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#aec7e8', '#ffbb78', '#98df8a']\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "ax.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90, colors=colors, wedgeprops={'edgecolor': 'black'})\n",
        "ax.set_title(\"Error Type Distribution across Models\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "H8QrguGLa0uV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}